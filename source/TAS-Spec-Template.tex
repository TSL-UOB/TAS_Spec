%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

\usepackage{enumitem}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmcopyright}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{On Specifying for Trustworthiness}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Name of Author 1}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 2}
\affiliation{%
  \institution{Institute}
  \streetaddress{Street address}
  \city{City}
  \country{Country}}
\email{Email}

\author{Name of Author 3}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 4}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 5}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 6}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 7}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 8}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 9}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 10}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 11}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

\author{Name of Author 12}
\affiliation{%
	\institution{Institute}
	\streetaddress{Street address}
	\city{City}
	\country{Country}}
\email{Email}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Author name, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
As autonomous systems are becoming part of our daily lives, specifying for trustworthiness of these systems is crucial. ...
In this article, we take a broad view of specification, concentrating on top-level requirements including but not limited to functionality, safety, security and other non-functional properties that contribute to trustworthiness. 
The main contribution of this article is a set of high-level intellectual challenges related to specifying a trustworthy autonomous system without focussing on how these challenges are actually realized. We also identify their potential uses in a variety of autonomous systems domains. ...
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010147.10010178</concept_id>
	<concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10011007.10011074.10011075.10011076</concept_id>
	<concept_desc>Software and its engineering~Requirements analysis</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10011007.10010940</concept_id>
	<concept_desc>Software and its engineering~Software organization and properties</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10011007.10010940.10010992</concept_id>
	<concept_desc>Software and its engineering~Software functional properties</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10011007.10010940.10011003</concept_id>
	<concept_desc>Software and its engineering~Extra-functional properties</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Software and its engineering~Requirements analysis}
\ccsdesc[500]{Software and its engineering~Software organization and properties}
\ccsdesc[500]{Software and its engineering~Software functional properties}
\ccsdesc[500]{Software and its engineering~Extra-functional properties}

%\ccsdesc[500]{Computing methodologies~Artificial intelligence}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
%\keywords{datasets, neural networks, gaze detection, text tagging}
\keywords{autonomous systems, trust, specification}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
An \textit{autonomous system} is a system that involves software applications, machines and people, which is capable of taking actions with no or little human supervision \cite{TAS-Hub}. 
Autonomous systems are no longer being confined to safety-controlled industrial settings. 
Instead, these systems are becoming increasingly used in our daily lives having matured across different domains, such as driverless cars, unmanned aerial vehicles, and healthcare robotics. 
As a result, specifying for \textit{trustworthiness} of these systems is crucial. 

According to ISO/IEC 2382:2015 standard, \textit{specification} is a detailed formulation that provides a definitive description of a system for the purpose of developing or validating the system \cite{ISO2382}. 
Writing specifications that capture trust is challenging \cite{Kress-Gazit2021}. 
A human trusts a robot to perform its actions, if it demonstrably acts in a safe manner. 
Here, the robot not only needs to be safe, but also needs to be perceived as safe by the human. 
In the same manner, in shared human-robot control situations, it is equally important to ensure that the robot can trust the human. 
This is particularly important in safety-critical scenarios when the robot needs to perform a task on behalf of the human where reasoning about trust is needed. 
In this regard, measuring human's current state (e.g. levels of fatigue, frustration) and assessing human's ability to actually perform the task are critical factors. 
To realize this, specification needs to consider formalisms that go beyond typical safety and liveness notions. 

Different research disciplines define \textit{trust} in many ways. 
Our study focuses on the notion that concerns the relationship between humans and autonomous systems. 
According to \cite{TAS-Hub}, autonomous systems are trustworthy depending on several key factors, such as:  robustness in dynamic and uncertain environments; assurance of their design and operation; confidence they provide; explainability; defences against attacks; governance and regulation of their design and operation; and consideration of human values and ethics.

In this article, we take a broad view of specification, concentrating on top-level requirements including but not limited to functionality, safety, security and other non-functional properties that contribute to trustworthiness. 
The main contribution of this article is a set of high-level intellectual challenges related to specifying a trustworthy autonomous system without focussing on how these challenges are actually realized. 
In this article, we discuss approaches for specifying trustworthy autonomous systems and identify their potential uses in a variety of autonomous systems domains. 
We conclude with a set of intellectual research challenges for the community.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Autonomous Systems Domains and Their Unique Challenges}
Each autonomous systems domain brings about unique specification challenges: 

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Autonomous Driving}
\textbf{[Responsible Author:  Subramanian Ramamoorthy]}\\
\textbf{[Source: Subramanian Ramamoorthy's presentation]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenge/s unique to this domain}}\\\\
Formal reasoning for "traffic regulation"...
Analysis of traffic regulation...
high-way code (e.g. when is it safe to make the turn and not to make the turn etc)...

The goal is to provide domain property analysis support for autonomous vehicles. 
1) To formalise structures and logic of traffic regulations; and 2) to allow breakdown of key environment variables for vehicle operations.

High way code is written in plain English. It is written reasonably precisely. It is not formally specified as such. These kinds of code needs bit of interpretation. So how do we approach specification. Currently, we are taking Dutch traffic laws as examples, i.e. WVW and RVV (will move to the UK highway code later)... 

The format of regulations defines constraints for the vehicle operations, e.g. overtaking and positioning. For each operation, there are two categories of constraints, 'must' (or 'prohibit', which is negation of 'must') and 'permit'.

Need to reason about exceptions, rule conflicts and open texture.

What are the scope of each of these rules, what are the preconditions, how do we want to deal with them, left implicit or needs further clarification in some other bit of the code. That is a specification challenge.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Emergency Responders}
\subsubsection{Emergency Responders} ...\\
\noindent\textbf{[Responsible Author:  Amel Bennaceur]}\\
\textbf{[Source: Amel Bennaceur's presentation] }\\\\
%\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenge/s unique to this domain}}\\\\
%\textbf{Emergency Responders: } 
%Why emergency scenario? Because by definition it is a disruption of usual operations. So they include a lot of unforeseen events and need for adaptation, and there is a requirement around the speed of response and adaptation of response to deal with critical situations...
%We need to specify functional and also SLEEC (social-legal-ethical-empathy-cultural) requirements. Those will change from one scenario to another. It is not only a design problem, there is a need for adaptation.
%
%Amel Bennaceur described a use case using drones or autonomous agents in general to assist users...
%Lot of work done in assisting first responders. These are fire fighters, doctors... 
%- 1:  Interact with emergency response team to optimize the emergency response...
%We are interested in how to make zero responders (passers by those who don't have training) help other people... 
%- 0: Coordinate human zero responders
%For that drones or autonomous agents move from simple sharing information with people into something like coordinating them, helping them collaborate.
%There is a very big requirement around how you communicate with those in order to ensure collaboration between them and collaboration with the autonomous agents. This actually to encourage pro-social behaviour. We want them to help. We want them to intervene in a pro-social way. Role of the autonomous  agent is to coordinate the response and help them collaborate. There is also another requirement: collaborate between autonomous agents or helping the whole system (coordinate with other agents)... 
%So to enable autonomous agents to interact with humans in these different ways is one of the things is to specify...(how to model humans to enable cooperation?)\\

An emergency is an inherently disruptive situation or series of events that interfere with usual operations and affect their efficacy. How they evolve, depends on the dynamic characteristics of emergencies. They differ in terms of the type of incident, its magnitude, additional hazards, the number and location of injured people. They are also characterised by urgency; they require a response in the shortest time frame possible and call for a coordinated response of emergency services and supporting organisations~\cite{cabinet2013emergency}. This means that successful resolutions depend not only on effective collaboration between humans~\cite{james2011organizational}, but also effective collaboration between humans and autonomous systems. Thus, for autonomous systems used in emergency operations, there is a need to specify both functional requirements (what the system does) and SLEEC requirements (the Social, Legal, Ethical, Empathic, Cultural rules and norms that govern an emergency scenario). This suggests a shift from a merely design challenge towards the need to specify for adaptation to the diversity of emergency actors and complexity of emergency contexts. For example, much research on autonomous systems for emergency situations has focused on operational aspects that assist first responders to make critical choices~\cite{graven2017managing}. However valuable its contribution, this research has overlooked another requirement that is integral to emergency scenarios: coordination and cooperation with zero responders. Social research has shown that zero responders play active role in emergency operations and are likely to help each other, as well as the first responder groups~\cite{drury2018role}. These pro-social behaviours need to be encouraged by autonomous systems. To enhance such collaboration between autonomous agents and different human agents in emergencies, specifying human behaviour remains one of the main challenges in emergency settings.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsubsection{Public Protection and Disaster Relief}...\\
\noindent\textbf{[Responsible Author:  Luke Moffat]}\\
\noindent\textbf{[Source: Luke Moffat's presentation]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenge/s unique to this domain}}\\\\
%\textbf{Public Protection and Disaster Relief:} Luke provided an example which was in the domain of public protection and disaster relief. 
%This was a project with a consortium of developers where we essentially did creative method based workshops to try and address (specify) what potential ethical issues might be in the field when using a Pan-European 5G network for public protection disaster relief (for emergency responders). For this we used creative exercises like what you see in this Figure. 
%During the lock down, we sent a series of exercises by post and then collaborated online. 
%The aim of this is not just to breakdown the monotony of doing online interaction, but also to see how addressing ethical issues which can be a creative exercise. Figure - designing and facilitating online/offline remote collaboration. 
\textbf{Public Protection and Disaster Relief:} 

There are various challenges for specifying security in the context of Public Protection and Disaster Relief. A large part of this comes from the vast amounts of data that need to be collected, shared and stored between different agencies and individuals during an emergency scenario. While autonomous systems can replace or adapt human involvement in some of the higher risk aspects of emergency response – largely on the ground – there still remains a need for human oversight of data collection and sharing.

Securing a collaborative information management system is divided between technical forms of security, such as firewalling and encryption, and social forms of security, such as trust. In order to properly provide security to a system, both aspects must be addressed in relation to each other, not one after the other or each one by separate committees. Even more, not having everyone on the same page about what security means and the responsibility to achieve and maintain it, can jeopardise the long-term security of a collaborative information management system.


%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Interactive Robot Systems}
\textbf{[Responsible Author:  Yiannis Demiris]}\\
\noindent\textbf{[Source: Updated by YD on 30 Oct 2021}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenge/s unique to this domain}}\\\\ 
\textbf{Trustworthy Interactive Robot Systems:} 

Trustworthy Interactive Robot Systems aim to complete their tasks while explicitly considering states, goals and intentions of the human agents they collaborate with, and aiming to calibrate the trust humans have for them to an appropriate level.  This form of human-in-the-loop real-time interaction is required in several application domains including assistive robotics for activities of daily living \cite{GaoEtAl2020}, healthcare robotics, shared control of smart mobility devices \cite{SohDemiris2015}, and collaborative manufacturing among others. Most specification challenges arise from the need to provide specifications for the perceptual, reasoning and behavioural processes of robot systems that will need to acquire models of, and deal with, the high variability exhibited in human behaviour. The necessity to infer human internal states (such as beliefs and intentions\cite{Demiris2007}) interactively, typically through sparse and/or sensor data from multimodal interfaces, imposes further challenges for the principled incorporation of human factors and data-driven adaptation processes in robots operating in close proximity to humans, where safety and reliability are of particular importance. 



%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Healthcare Robotics}
\textbf{[Responsible Author:  Subramanian Ramamoorthy]}\\
\noindent\textbf{[Source: Subramanian Ramamoorthy's presentation]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenges unique to this domain}}\\\\
\textbf{AI-based Medical Diagnosis:} Subramanian Ramamoorthy mentioned that from the ground up, it is a machine learning problem. 
An example: cataloging the findings - a radiologist's process...This is based on automated interpretation of radiology imaging...
 
What you find best models in terms of machine learning is subsymbolic neural networks-based models. What the models are looking for is essentially small cues integrated together through some sort of  probabilistic fusion or other forms of integrating the information towards an overall diagnosis. This is very different from how the human experts are today being trained. A neural network is picking up cues completely different from that. So if you want to make a specification we have to first understand what is going on, and also the ways in which these systems fail (e.g. machine learning not picking up images accurately)... you need to pay attention to right cues. But you do not know what precisely to specify because all the measures that I apply in the model such as accuracy, recall and precision and sensitivity,  they all can be fooled if you don't get it right...

Deep learning has revolutionised what is possible in terms of segmentation, classification and prognostics. However, there are many caveats!
Usual and unprecedented forms of edge cases, e.g. did the model even look for disease in the first place?!

How to specify with respect to black box models?:
1) The design domain for operation and notions of coverage: camera properties. model features, semantics of biological features; 2) role of explainability and faithfulness of interpretation of semantics; 3) role of pre-trained models in pipelines

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Swarm Robotics}
\textbf{[Responsible Author:  Dhaminda Abeywickrama]}\\
\textbf{[Source: Dhaminda Abeywickrama's presentation]} \\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenge/s unique to this domain}}\\\\
%Previous investigations have shown that users are open to adopting swarm robotic solutions, if the swarms are implemented in a trustworthy manner. 
%For example, let us take an example of a case study that describes a public cloakroom where swarm of robots assist customers looking to deposit their jackets at an event. Specification challenges...

Swarm robotics provides an approach to the coordination of large numbers of robots, which is inspired from the observation of social insects \cite{Sahin2005}. 
Three desirable properties in any swarm robotics system are robustness, flexibility and scalability. 
In \cite{Sahin2005}, a set of criteria is proposed for distinguishing swarm robotic research from multi-robot systems. 
The individual robots that make up the swarm need to be autonomous. Also, there need to be a large number of robots; a few homogenous groups of robots; relatively incapable or inefficient robots; and robots with local sensing and communication capabilities. 

The functionality of a swarm is emergent (e.g. aggregation, coherent ad hoc network, taxis, obstacle avoidance and object encapsulation \cite{Winfield2006}), and evolves based on the capabilities of the robots and the numbers of robots used. 
The overall desired behaviours of a swarm are not explicitly engineered in the system, but they are an emergent consequence of the interaction of individual agents with each other and the environment.
This evolving functionality across many individual robots poses a challenge in how to specify a swarm that is safe, trustworthy by design, useful, and acceptable to users?
%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Unmanned Aerial Vehicles}
\textbf{[Responsible Author:  Dhaminda Abeywickrama]}\\
\textbf{[Source: Dhaminda Abeywickrama's presentation]} 
\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenge/s unique to this domain}}\\\\
%Today the operation of UAVs in applications like parcel delivery is very challenging with complex and uncertain flight conditions (such as wind gradients), and highly dynamic and uncertain air space (such as other UAVs in operation). Specification challenges....

A unmanned aerial vehicle (UAV) or drone is a type of aerial vehicle that is capable of autonomous flight without a  pilot on board.
As UAVs are increasingly being applied in diverse areas of applications \cite{Ukaegbu2021}, such as logistics services, agriculture, emergency response, and security, ensuring their trustworthiness is crucial.  
Specification of operational environments of UAVs is challenging mainly for two reasons. 
First, the unclear and uncertain government regulations make it challenging, as these rules may change over time. 
Second, the complexity and uncertainty of the operational environment of the UAV itself is a challenge. 
For example, in parcel delivery using UAVs in urban environments, we could mention uncertain flight conditions (e.g. wind gradients), and highly dynamic and uncertain airspace (e.g. other UAVs in operation).

The recent advances in machine learning offer the potential to increase the autonomy of UAVs by allowing them to learn from experience. For example, machine learning can be used to stabilize the flight which can greatly improve performance in gusty urban wind conditions. 
When one considers the conventional flight controller of a UAV, it can include several measures, such as risetime, overshoot, settling time, and steady-state error \cite{Kada2011}.
The goals of these specification measures are to ensure that the control system is stable and robust; disturbance attenuation against environmental uncertainty; smooth and rapid responses to set-point changes; and steady-state accuracy. 
In this context, a key challenge is how do we specify a UAV should deal with situations that go beyond the limits of its training?

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Policy Development}
\textbf{[Responsible Author:  Luke Moffat]}\\
\textbf{[Source: Luke Moffat's presentation]} 
\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 150-300 (maximum); \\Format/structure: introductory sentences on the domain; specification challenge/s unique to this domain}}\\\\
%Project Luke Moffat works with TAS-S Node...
%isITethical...
%Ethical framework adapted to secure TAS framework where both we aim to work with engineers, designers, programmers, policy advocates and publics and non-governmental organizations. To get many voices in to the same room as possible in thinking about not just how these devices or technologies should be used when they arrive, but how these devices should be designed in the first place.. whether they are worthy of our trust and whether they are worthy of their use.

Policy can be seen by research domains as slow moving, and hard to penetrate. Beyond policy recommendations as research outcomes, what channels exist for informing policy development around autonomous systems? Through the \textit{isITethical} platform \cite{isITethical}, some work has been completed in collaboration with engineers, designers, programmers, policy advocates and publics and non-governmental organizations. One legacy from this work that has transferred to TAS-S, is the use of creative and participatory methods to facilitate conversations and ways of thinking about ethics and security in the future of AS operations. These methods can help to recognise and sustain a need to get many voices in to the same room as possible, in order to think about not just how these devices or technologies should be used when they arrive, but how these devices should be designed in the first place. Instilling a sense of ethics through security, that is maintained through the design process, can generate good use cases that inform policy and standards the endure in future AS development. We must the think about whether AS are worthy of our trust, and also whether we are worthy of their use.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\section{Intellectual Challenges for Research Community}
Now we discuss intellectual challenges for the research community towards creating trustworthy autonomous systems... \\

\noindent[\textit{Note: In order to guide the initial writing process, we have organized the different contributions to different subsections.}]

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{TAS Regulation and Governance}
\noindent\textbf{[Responsible Author:  Subramanian Ramamoorthy]}\\
\noindent\textbf{[Source: Subramanian Ramamoorthy's presentation]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\\\
Two issues most salient within the two case studies described previously are:
\begin{itemize}
	\item \textbf{Environment and other agents' dynamics can be unknown, but crucial}
	\item \textbf{How to establish real intent behind requirements, vagueness and preferences?}
\end{itemize}

At the high-level the issue for us is the gap between formal specification way of thinking what systems got to do and how everybody else think systems should be thought of...if you take the definitions of fairness, there can be twenty valid definitions...

How and why are AI-based systems different?... we are concerned more specifically about data-driven machine learning methods, more specifically neural networks etc that are becoming the norm...

Shift from specification -> design to data -> specifications many gaps!...task is implicitly specified in data and specifications are implicit inside that data, or derived from that data....if you take the design processes that involve continuous deployment pipeline, it is challenging traditional modes of regulation...The issue in some of the models is inscrutability with respect to performance, error characteristics, etc.; contingency in any statements about behaviour.

To try to be more specific, 1) environment and other agents' dynamics can be unknown, but critical; 2) how to establish real intent behind requirements, vagueness and preferences...
We are interested in open environments and open systems. There are other decision makers or entities in our environment and we have to take them into account. Typically these other environments are humans or machines operated by humans. This means we don't have very good models of what they do. We have to figure out how to specify something in the absence of this....conservative approximations do not make sense in domains like autonomous vehicles... conservative approximations can be too conservative and not very useful functionality wise. Other issue is the real intention is vague (fairness which is not vague, but it is difficult). It is hard to pin it down and when we try to pin it down, it does not capture what we try to say...these are high-level issues.

\noindent\textbf{[Source: Subramanian Ramamoorthy's abstract of talk]}\\
Computer scientists treat specifications as precise objects, often derived from requirements by purging features such that they are defined with respect to environment properties that can be relied on regardless of the machine's behaviour. Emerging autonomous systems applications can sometimes challenge this way of thinking, particularly so because the environment properties may not be fully understood, or because it is hard to establish if the real intent behind a requirement can be verified. These gaps should be addressed in governance frameworks. I'll try to quickly develop this idea using two use case examples - one from autonomous vehicles, and another from AI-based medical diagnostics.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{TAS Functionality (Specification)}
\noindent\textbf{[Responsible Author:  Dhaminda Abeywickrama]}\\
\noindent\textbf{[Source: Dhaminda Abeywickrama's presentation]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
\begin{itemize}
	\item \textbf{On standards for autonomous systems with evolving functionality}
\end{itemize}

Autonomous systems with \textit{evolving functionality} (i.e. the ability to change in function over time) pose significant challenges to current processes for specifying functionality. 
Most conventional processes for defining system requirements and characteristics assume that these are fixed and can be defined in a complete and precise manner before the system goes into operation. 
Existing standards and regulations do not satisfactorily address issues raised by autonomous systems with evolving functionality, which is a key limitation \cite{Fisher2020}. We discuss this using two application areas of autonomous systems -- swarm robotics and UAVs.

In the robotics domain, several safety standards have been developed by a technical committee of the ISO for the non-industrial (service) robotics sector (e.g. ISO 13482 \cite{ISO13482}, ISO 23482-1 \cite{ISO23482-1}, ISO 23482-2  \cite{ISO23482-2}), as well as for the industrial robotics sector (e.g. ISO 10218-1\cite{ISO10218-1}, ISO 10218-2\cite{ISO10218-2}, ISO/TS 15066 \cite{ISO15066}). 
Different legal and regulatory requirements apply to different robot categories. 
In service robotics, ISO 13482 covers the hazards presented by the robots and devices for applications in non-industrial environments for providing services. 
ISO 23482-1 and ISO 23482-2 standards extend ISO 13482 with guidance and methods that can be used to test personal care robots.
On the other hand, in the industrial sector, ISO 10218-1 and ISO 10218-2 standards provide safety requirements for industrial robots and their integration.
Meanwhile, ISO/TS 15066 provides safety requirements for collaborative industrial robot systems
and work environment. 
Although these industry standards focus on ensuring safety of robots at the individual robot level, there are no standards to ensure safety or any other extra-functional property for \textit{swarms}.

Meanwhile, for the airborne systems and in particular for UAVs, several industry standards and regulations have been introduced to ensure their safe operation. 
DO-178C \cite{DO-178C} is the primary standard for commercial avionics software development, which provides software considerations for the production of airborne systems and equipment. On the other hand, DO-254 \cite{DO-254} provides guidance for the development of airborne electronic hardware. ED279 \cite{ED279} standard provides a framework to support designers when performing a functional hazard assessment process for an unmanned aircraft system. 
ARP4761 \cite{ARP4761} provides guidelines and methods for conducting the safety assessment process on civil airborne systems and equipment. 
NATO STANAG 4671 \cite{STANAG4761} is intended to allow military UAVs to operate in other NATO members airspace. 
As for regulations within the UK, CAP 722 \cite{CAP722} is the primary guidance document for the operation of unmanned aircraft systems. 
However, none of these standards or regulations provide safety considerations for machine learning components that enable evolving functionality.

Furthermore, when one considers the existing industry standards, they are either implicitly or explicitly based on the V-model, which moves from requirements through design onto implementation and testing \cite{Jia2021}. 
For systems with the ability to adapt their functionality, this approach is unlikely to be suitable. 
For instance, the development of machine learning-based components of a UAV follows a different, much more iterative life-cycle. Therefore, there is a need for new standards and guidance that better reflect the life-cycle of autonomous systems with evolving functionality \cite{Hawkins2021}.
%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Explainability by Design} %%% I would prefer a mention of explainability requirement, as i don't think i speak on behalf of the TAS HUB
\noindent\textbf{[Responsible Author:  Luc Moreau]}\\



Increasingly, calls for explainability~\cite{Hamon:2020,gchq:2021}, as well as  emerging frameworks~\cite{Hamon:2020} and guidance ~\cite{ico:2020}  point to the need for Artificial Intelligence (AI) to provide explanations about the decisions it makes.  A challenge with existing frameworks and guidance is that they are not prescriptive: what is an actual explanation?   how   should an explanation be constructed? can it be constructed  computationally?  Furthermore,  frameworks and guidance tend to be concerned with AI in general, and not Autonomous Systems, which have a substantial overlap with AI but are distinct from AI.


A case study addressing regulatory requirements on explainability of automated decisions in the context of a  loan application~\cite{Huynh:DGOV21} provided foundations for a systematic approach. Within this context, explanations  can act as external detective controls~\cite{Tsakalakis:CLSR21}, as they provide
specific information to justify the decision reached and help the user take corrective actions: ``The aim of these explanations is to provide data subjects with an adequate understanding about the decision so that they can exercise their corrective rights''~\cite{Tsakalakis:CLSR21}. But explanations can also act as internal detective controls~\cite{Tsakalakis:CLSR21}, i.e.,  a mechanism for organisations to demonstrate compliance to the regulatory frameworks they have to implement. 
Both~\cite{Huynh:DGOV21} and~\cite{Tsakalakis:CLSR21} provide   systematic questions that an explanation designer needs to consider: What is the purpose of an explanation? What is the audience of an explanation?  What is the information that it should contain? However, to adequately address these questions, explainability should not be seen as an after-thought, but as an integral part of the design of an application, leading to explainability requirements to be included in its design, like all its other aspects.  


The PLEAD project (\href{https://plead-project.org/}{https://plead-project.org/}), ``Provenance-based and Legally-grounded Explanations for Automated Decisions'' project adopts a two-pronged approach to address this challenge. (1) Technologically, PLEAD decomposes the problem of explainability in two distinct parts,   logging “a record that describes the people, institutions, entities, and activities involved in producing, influencing, or delivering”~\cite{Moreau:prov-dm:20130430} an automated decision, and using that record to construct ``An explanation, as a targeted, personalised textual or visual artifact that provides a focused description of the behaviour of the system''.  A structure for this record already exists in the form of  PROV, a Recommmendation of the World Wide Web Consortium~\cite{Moreau:prov-dm:20130430}.  (2) Methodologically, PLEAD offers a socio-technical approach, involving autonomous systems stakeholders to understand various dimensions of an explanation (purpose, audience, trigger, minimum content, explanation examplar, ...), to which an ``explainability by design'' methodology is applied to construct the system able to generate explanations that can be evaluated by stakeholders, before  deployment and integration with the application.

Explainability by design is a methodology that takes the stakeholder requirements for explainability as input, models application provenance, design queries, and constructs explanation plans, which can be all bundled up in a standalone component able to receive applications data and generate explanations addressing the requirements identified by the PLEAD socio-technical analysis.


We have applied the methodology to systems providing explanations about decisions related to credit card applications and school allocations. These decisions are taken with various degrees of autonomy, in partnership with humans.  Preliminary evaluations are showing positive feedback from the participants.

In the context of Trustworthy Autonomous Systems, emerging AS regulations could be used to drive the socio-technical analysis of explainability.  A particular emphasis would have to be on the autonomy and the hand-over between systems and humans that characterise  TAS systems.  The audience of explanations will also be criticial, from users and consumers to businesses, organisations and regulators.  Finally, considerations for post-mortem explanations, in case of crash or disaster situation involving autonomous systems, should lead to adequate architectural design.


%% \noindent\textbf{[Source: Luc Moreau's presentation]}\\\\
%% \noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
%% \begin{itemize}
%% 	\item \textbf{How do you incorporate explainability requirements in autonomous systems designs?}
%% \end{itemize}
%% Luc Moreau said we need to think about explainability from the start when we specify and design a system. This is illustrated using the animation (plays a video)...Every day organizations use computers to make decisions that impact our lives. These could be life defining decisions like what schools our children are allocated to or whether our mortgage application is accepted or rejected. What if the computer says no? We have the right to challenge these decisions and to understand whether they were fairly made. What was the provenance of the decision (what data was used)? Where did that data come from? How was it used to make the decision? Was the decision fully automated without any human involvement? In response to growing demand for transparency and accountability around automated decision-making, PLEAD is developing the Explanation Assistance. It turns descriptions of the flow of data in a decision making system into meaningful explanations. The Explanation Assistant helps organizations demonstrate that they comply with legal regulations and makes it easier to explain complex decisions to their customers reassuring them that their data was processed as expected. More transparency improves customer satisfaction, a win-win for all. End of video.

%% Luc Moreau introduced the PLEAD project which is done by an interdisciplinary team. 
%% An explanation is a targeted, personalised textual or visual artifact that provides a focused description of the behaviour of the system based on data, processes, people and organisations that have influenced an outcome or a decision. 
%% Targeted: addressing a specific purpose for the individual(s) targeted; personalised: referring to circumstances of the individual(s) targeted; description: explanation describes aspects of a system's behaviour. 

%% Why provenance?...
%% WWW consortium defines provenance as: provenance is a record that describes the people, institutions, entities and activities, involved in producing, influencing, or delivery a piece of data or a thing in the world. W3C PROV - provides a strong foundation to derive explanations from the provenance of decisions. 

%% PLEAD methodology overview....
%% Methodology steps: application, explanation requirements (policy analysis, requirements classification, minimum content determination), application data (collection of data categories, comparison to data flows), explanation plans, modelled provenance, explanations, explanation assistant. 
%% Explanation requirements: policy analysis, requirements classification, minimum content determination...
%% Each step is described by small card. 
%% Selected (sub-)steps: analyse policy requirements, apply classification framework, determine minimum content that we need to find in explanations, compare to data flows in a system, explanation plans, and explanation validation. This is all packaged in the Explanation Assistant we described earlier.

%% In conclusion, "Explainability by design" is crucial for TAS. We are writing up the methodology (from computer science and legal perspectives). We are still collecting data (interviews, costs etc). We are interested in exploring further applications.

%% What about TAS? What are the legal/regulatory requirements or business needs to be met? Apply PLEAD methodology. Incorporate explainability requirements in TAS designs. 

%% \noindent\textbf{[Source: Luc Moreau's abstract of talk]}\\
%% Explainability of computer-based systems, and specifically autonomous systems, is a key aspect to make them trustworthy. We argue that explainability is not an afterthought, but it needs to be weaved in the application design.

%% In the PLEAD project (Provenance-based and Legally-driven Explanations for Automated Decisions), we have designed a socio-technical methodology to elicit explanation requirements from laws and regulatory frameworks, to specify explanations with a clear purpose targeting a specific audience, and to construct explanations programmatically by means of explanation plans and queries over provenance logged by the system being designed.

%% We have applied the methodology to systems providing explanations about decisions related to credit card applications and school allocations. These decisions are taken with various degrees of autonomy, in partnership with humans.  Preliminary evaluations are showing positive feedback from the participants.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{TAS Resilience}
\noindent\textbf{[Responsible Authors:  Amel Bennaceur \& Anastasia Kordoni]}\\
\noindent\textbf{[Source: Amel Bennaceur's presentation]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
\begin{itemize}
	\item \textbf{How to specify humans to enable cooperation with autonomous systems?}
	\item \textbf{How to specify identities for human-autonomous system cooperation?}
\end{itemize}
%In this talk, Amel Bennaceur concentrated on one specific challenge and one specific example (application) in resilience.
%Challenge: Corporation between autonomous systems and humans. The context (example): resilience in emergency scenarios. 
%
%On specification challenge: How to specify (how to model humans to enable cooperation)?
%Look at from a behaviour view (behaviourist of the user), as an automata for example. But, this is a very restrictive way of specifying how a human behaves. There is uncertainty about the actions people can do. Humans don't behave the way we expect them to, also the computers also don't behave the way we expect them to...So there is a kind of evolution to an ontological view (from behaviourist view) where you don't want to consider actions but you also want  (i) opportunity the user can have (e.g. user is less than 5m away); (ii) their willingness to take certain actions; (iii) capability of each user (e.g. user can hear the robot). Here specifying little more details... But, this ontological view is still not enough, as it does not show the dynamics between different people. 
%
%So there is another way, we started to looking at which from a "Joint cognition perspective". It is not only how the autonomous system sees the system, also it is important to understand how the human sees the autonomous agent. This may help in specifications, such as explainability...
%Joint cognition: 1) e.g. user understand how robots behave and is willing to cooperate; 2) could it be translated into other variables (e.g. social identity markers, cooperation index)...
%Cooperation is not only a reaction to actions. Actually, it goes further. There is a joint cognition and there is almost a social identity shared between the autonomous agents and humans. Its is a matter of understanding what are the markers for this identity and for this com..??
%To sum up, there is multiple ways for looking at cooperation between humans and autonomous agents...Behaviour is only one facet. A view only on behaviour is not enough. Need for trade off between multiple requirements... lot of questions about what are the markers we need, how do we reason about them, and how we make trade off between multiple requirements such as safety, explainability... what to build trust in autonomous systems.
%
%\noindent\textbf{[Source: Amel Bennaceur's abstract of talk]}\\
%REASON aims to expand the ability of autonomous agents to cooperate with peer agents and humans, and to proactively seek such cooperation for mutual benefit—opening up significant new opportunities for enhanced resilience through the pooling of information and functional capabilities. While significant progress has been made for specifying and enabling cooperation between autonomous systems, cooperation with humans raises additional challenges which this talk explores.
%
%\noindent \textbf{[Source: Breakout Room-2 (Amel Bennaceur, Anastasia Kordoni, Adrian Bodenmann, Kerstin Eder)]}\\
%Amel Bennaceur to summarise the outcome of their breakout room mentioned: we went deeper to corporation.. once you want to model the corporation what are the highlights? So they talked about... there is lot of social theories that specifically the identity so how to represent it, how to understand it, how to translate it  to something that computer scientists understand and can implement. 

How to model human behaviour to enable cooperation with an autonomous systems is challenging but crucial for the resilience of the system as a whole. It is the diversity in human enactment that drives the uncertainty about what people do and don’t do, and, subsequently, the way human behaviour can be specified. Knowing the mental state of others enables autonomous systems to steer a cooperation that is consistent with the needs of the autonomous systems, as well as to respond to the needs of human agents in an appropriate manner. This knowledge guides an optimal course of action in a given context and can be used to identify mutually beneficial solutions. 
Different theories of human behaviours explain diversity in human action in different ways and by detecting various determinants of human behaviour. For example, a behaviourist approach suggests that every behaviour is a response to a certain stimulus~\cite{heimlich2008understanding}. Albeit true, this approach is restrictive in addressing the complexity of human behaviour~\cite{taylor2021explanation}, as well as the different ways that human behaviour develops during cooperation. To grasp that humans are embodied with purposes and goals that affect each other, the concept of joint-action was introduced as ``a social interaction whereby two or more individuals coordinate their actions in space and time to bring about change in the environment''~\cite{sebanz2006joint}. Adapting it to human-robot interaction, this approach is suggestive of an interplay between humans and autonomous systems, such that what matters is not only how the autonomous system understands the system but also how humans understand the way the autonomous agent behaves and is willing to cooperate. Thus, cooperation does not result from a reaction to actions but from a shared understanding between agents. 
The social identity approach~\cite{spears2021social} induces this concept of a shared understanding by providing an explanation of human behaviour focusing on how social structures act upon cognition. It proposes that, alongside our personal identity, our personality-who we are, we also have multiple social identities based on social categories and groups. We act in line with our social identity when this social identity is salient at a particular time~\cite{tajfel1982social}. Previous research has shown that social identities influence people’s relation with technology~\cite{lee2001effect}. Sharing a social identity initiates pro-social behaviours, such as helping behaviours in emergency situations~\cite{drury2018role}. People adapt their behaviour in line with their shared identities, which in turn, enhances resilience. Specifying social identities to enable cooperation is challenging. It requires an answer to a series of yet to be answered questions. How do we represent different identities and how do we reason about them? Following the social identity approach to specify identities for human-autonomous agent cooperation requires an investigation of how to operationalise social identity, i.e.~a psychological state, into software embedded within autonomous systems.



%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{TAS Trust}
\noindent\textbf{[Responsible Author:  Yiannis Demiris]}\\
\noindent\textbf{[Source: Updated by YD on 30 Oct 21]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
\begin{itemize}
	\item \textbf{How to incorporate human factors and data-driven adaptation processes where safety and reliability are of particular importance?}
\end{itemize}

Designing, implementing, and deploying interactive robot systems that are trustworthy for use in scenarios where humans and robots collaborate in close proximity is challenging, given that safety and reliability in such scenarios are of particular importance. Example scenarios include assisting people with activities of daily living such as mobility \cite{SohDemiris2015} and dressing\cite{GaoEtAl2020}, rehabilitation robotics, adaptive assistance in intelligent vehicles, robot assistants in care homes and hospital environments, among others. The intellectual challenge this research community faces is the design and implementation of trustworthy perceptual, cognitive, and behaviour generation processes that explicitly incorporate parametrizable models of human skills, beliefs, and intentions\cite{Demiris2007}. These models are necessary for interactive assistive systems since they need to decide not only how but also when to assist\cite{Demiris2009, GeorgiouDemiris}.  Given the large variability of human behaviour, the parameters of these user models need to be acquired interactively, typically from sparse and potentially noisy sensor data, a particularly challenging inverse problem. An additional challenge is introduced in the case of long-term human-robot interaction, where the assistive system needs to learn and take into consideration human developmental aspects, typically manifested in computational learning terms as model drift. As an example, consider the case of an assistive mobility device for children with disabilities \cite{SohDemiris2015}: as the child’s perceptual, cognitive, emotional and motor skills develop over time, their requirements for the type, amount and frequency of the provided assistance will need to evolve. Similarly, when assisting an elderly person or someone recovering from surgery, the distributions of the human data that the robot sensors collect will vary not only according to the context but also over time, for example when assisting children to learn about and manage chronic conditions such as diabetes \cite{KapteinEtAl2022}.  Depending on the human participant, and their underlying time-varying physiological and behavioural particularities,  model drift can be sudden, gradual, or recurring, posing significant challenges to the underlying modelling methods. Principled methods for incorporating long-term human factors into the design and implementation of assistive systems, that adapt and personalise their behaviour for the benefit of their human collaborator, remain an open research challenge. 



%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{TAS Security}
\noindent\textbf{[Responsible Author:  Luke Moffat]}\\
\noindent\textbf{[Source: Luke Moffat's presentation]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
%\begin{itemize}
%	\item \textbf{How to reach the middle ground between the technical possibilities and innovations related to the actual systems and social perspective?}
%\end{itemize}
%Main challenge in terms of specification is how to reach this middle ground between the technical possibilities and innovations related to the actual systems and the social perspective which engages with that technical landscape in a way that can be understood and digested by the publics who will end up using the technology. 
%
%Luke Moffat goes through how he has used the ELSI framework in the past with some examples.
%
%TECHNICAL - Systems:
%
%On the technical side, there are fairly specific definitions for specification which you all know. 
%From my perspective, the interested ones concern is where data is shared between systems what we called as social-material interactions... that is whenever an autonomous system communicates with a human being or an aspect of the environment and these have technical answers. So there is a certain ways you can specify how those practices operate on the technical level, but to understand how they interact with the social level requires collaboration. 
%
%How do autonomous vehicle systems communicate with other networked systems, with users, and with the environment?
%
%SOCIAL - Actors:
%
%So as soon as you introduce human-beings as in the figure, things get complicated, now you are not just dealing with instructions that you give to a device but dealing with believes, desires and fears sometimes misinformation so he is trying to understand how  autonomous vehicles (he is working with Highways England), how they are understood, regarded and perceived by publics. And the way in which pedestrians can be considered as passive users of autonomous vehicles. 
%
%How are autonomous vehicles regarded by publics, and how are pedestrians involved in automated mobility?
%
%TECHNO-SOCIAL (approach) - Accountabilities:
%
%What are the ethical challenges? also, the legal and social ones. 
%So how can you have regulation but also a social space that is responsive to new technologies in a way that is neither simply techno-phobic nor passively accepting but something that involves innovation and public input. So, the technology that you receive is what works for everyone. So, how you make ethical and accountable autonomous systems?
%
%Engagements: 
%An answer to this is to engage with many stakeholders as possible throughout the design process... ELSI is one method of doing it. Other methods: creative methods, ethics through design...
%
%\noindent\textbf{[Source: Luke Moffat's abstract of talk]}\\
%As autonomous systems become increasingly complex and pervasive, the task of ensuring their security and trustworthiness can be aided by considering ethical, legal, and social implications (ELSI) of AS use. In this presentation, I outline some of the challenges and opportunities in combining technical specification and social specification.
%
%Sharing some methods used by isITethical Exchange, a community platform for emergency response and disaster and risk management, I offer some possibilities for tackling the complex and unpredictable social dimensions in which AS come to operate. I advocate for participatory specification, drawing on multiple voices and perspectives to ensure that technical specifications of AS can also incorporate what is ethical, socially responsible, and legally warranted.

There are technical sides to security, but there are also social dimensions that matter when thinking through how an AS solidifies its status as secure. In this context, security overlaps with trust. One can only be assured a system is secure if one trusts that system. Public trust is a complex issue, shot through with media, emotions, politics, and competing interests. How do we go about specifying security in a social sense? 

On the technical side, there are fairly specific definitions for specification which can be grasped, and more, measured. From the social perspective, the possibility of specification relies on a network of shared assumptions and beliefs that are difficult to unify. In fact, much of the value from engagement over social specifications, derives from the diversity and difference. A predominant concern in social aspects of security is where data is shared between systems, what we call social-material interactions... that is whenever an autonomous system communicates with a human being or an aspect of the environment. As already mentioned, these interactions have technical answers. To get at the answers that speak to social science perspectives, requires collaboration, and agile methods to facilitate that collaboration.

As soon as you introduce human-beings as in the picture of Autonomous Systems, things get complicated, now you are not just dealing with instructions that you give to a device but dealing with believes, desires and fears sometimes misinformation, how those are understood, regarded and perceived by publics. For example, in what ways can we regard pedestrians as passive users of autonomous vehicles. How are autonomous vehicles regarded by publics, and how are pedestrians involved in automated mobility?

What ethical challenges emerge for AS security also relate to the legal and social ones. The difficultly centers around how to create regulation and specification on a technical level, but also useful for social spaces, responsive to new technologies in a way that is neither simply techno-phobic nor passively accepting. Doing so must involve both innovation and public input. So, the technology that you receive is what works for everyone. To engage designers, engineers, and public bodies in answering these questions, we are using the ELSI framework. Standing for Ethical, Legal \& Social Implications, this is an inherently cross-disciplinary set of approaches for tackling AS security as many interrelated and entangled aspects. Specifying security requires connection, collaboration, and agile ethical methods.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{TAS Verifiability}
\noindent\textbf{[Responsible Author:  Jan Ringert]}\\
\noindent[\textbf{Source: Jan Ringert's presentation}]\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\

The vision of the verifiability node is to enable domain experts to use the most suitable abstractions, specifications, and tools for the verification task at hand. 
Many aspects of autonomous systems, e.g., functionality, security, or reliability, require their own formalisms and abstractions. Existing tools work with particular abstraction to provide verification results. 
However, we must be able to verify different aspect of these systems and how they operate together to enable trust:
Our goal is to develop a \textbf{unifying framework that will integrate and coalesce rigorous verification techniques of autonomous systems to quickly and easily verify complex autonomous systems}. 
We will achieve this outcome through a multi-disciplinary technical \textbf{research  programme} that seeks to: 

\begin{enumerate}
  \item[\textbf{(a)}] \textbf{Classify} and relate verifiability concepts, notations, and techniques for autonomous systems.
  \item[\textbf{(b)}] \textbf{Design} the algorithms and techniques to integrate verification  across disciplinary domains.
  \item[\textbf{(c)}]  \textbf{Create} an ecosystem of verification techniques concerning aspects such as functionality, resilience, security, and ethics. 
\end{enumerate} 


To achieve this flexibility the verifiability node is developing a unifying framework for specification languages, their semantics, compositions, and transformations. 

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Incompleteness of Specifications}
\noindent\textbf{[Responsible Authors:  Jan Ringert \& James Wilson]}\\
\noindent \textbf{[Source: Breakout Room-3 (Jan Ringert, James Wilson, Carlos Gavidia-Calderon, Luke Moffat, Dhaminda Abeywickrama)]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\

Incompleteness is a necessary property of specifications. Only the use of suitable abstractions allows for coping with the complexity of systems~\citep{Kramer07}. However, there is an important difference in the incompleteness introduced by choosing abstractions, e.g., focusing on behavioral, structural, or security-related aspects of a system, and incompleteness with respect to the purpose of the specification, i.e., the faithful representation of the system given the chosen abstraction\footnote{The case where a chosen abstraction does not fit the purpose of the specification, e.g., the purpose is to rule out collisions of robots, but the abstraction ignores actuator inaccuracies, is out of the scope of this discussion.}. 

On the one hand, if the purpose of creating and analysing a specification is to explore a system and to learn about possible constraints, e.g., as done with lightweight formal methods tools like Alloy~\cite{Jackson19}, then incompleteness of the representation in the specification is important as it allows for flexibility and quick gains. On the other hand, if the purpose of the specification is to prove a property and certify a system, then incompleteness of the representation may lead to incorrect analyses results. These incorrect analysis results may lead to false positives or false negatives. False positives can usually be fixed by adding the missing knowledge to the specification. As an example for a false positive consider the specification of an insulin pump from~\cite{HarrisonMCC17} and the property that all actions are reversible. The specification reported a false positive due to an advanced handling of overflows in the insulin pump and the specification had to be extended.

In addition to analysis tasks, specifications are also used in synthesis tasks. In those cases the incompleteness of specifications may manifest in building biased or incorrect systems. Prominent examples include the bias of AI systems making autonomous decisions in health care~\cite{science.aax2342}. Other examples were observed in the area of program repair~\cite{GazzolaMM19} where initial work mainly relied on sets test cases (which in many cases represent incomplete specifications) producing incorrect repairs~\cite{QiLAR15,SmithBGB15}. 

More general, the synthesis of autonomous systems from incomplete specifications may simply fail if assumptions on the environment are incomplete. As an example, consider a simple robot operating in a warehouse. The robot has sensors to detect obstacles and the specification requires the robot to never hit a wall. The synthesizer would report that the specification is not realizable and no implementation exists. The worst-case expectation of synthesis, if nothing else is specified, would be for walls to move and hit the robot. The environment needs to be further constrained by adding the natural assumption that walls cannot move\footnote{Additional challenges of writing specifications for reactive synthesis are described in~\cite{MaozR18}.}. In the case of verification we can similarly expect a false positive result without this assumption. 
Interestingly, when formulating requirements to be read and understood by humans one would likely not formalize these assumptions necessary in specifications for autonomous systems. 

To summarize, one challenge arising from automated analysis of specifications is to capture and encode implicit knowledge and common understanding available to humans into formal specifications. This includes to detect issues that were forgotten to specify 
%(without relying on false positives and then fitting the specification to the implementation) 
and to disambiguate multiple plausible interpretations.
%(without extensive simulation and updating the specification only when incorrect behavior was discovered).

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Evolution of Specifications}
\noindent\textbf{[Responsible Authors:  Jan Ringert \& James Wilson]}\\
\noindent \textbf{[Source: Breakout Room-3 (Jan Ringert, James Wilson, Carlos Gavidia-Calderon, Luke Moffat, Dhaminda Abeywickrama)]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\

Every non-trivial system undergoes changes over its lifetime. The evolution of trusted autonomous systems may concern changes of the requirements of their functional or non-functional properties, changes of the environment that it operates in, and changes in the trust of users towards the system. Initial specifications of the system may no longer reflect the desired properties of the system and fail to represent its actual environment.

Self-adaptive systems~\cite{SalehieT09} constitute a special case of evolving systems where adaptation of the system is initiated from within itself. These systems require a tight coupling between the specification and implementation.

Pursuing particular methods of evolution can make the task of specification easier. Some approaches, such as \cite{francesca2014automode}, allow systems to evolve behaviours from sets of predefined modules. These sub-modules can be independently specified and once the behaviour is evolved these independent specifications could be aggregated to form a valid overarching specification for the system. Similarly, adaptation can be limited to a number of specified behaviour states or traits which are modified reactively \cite{wilson2019amalgamation}. If configured effectively, such systems should remain within the envelope of initial specification as the system evolves or adapts within its given constraints. Designing evolvable systems in this way will not only make specifying a system an easier task but will also make the system more transparent, understandable and potentially more trustworthy.

While a lot of work has been done to analyse, repair, explain, or simulate specifications little work has been to compare specifications, detect differences, and analyse their impact on the specified system. The comparison of complex specifications where small syntactic changes may have a large impact on their semantics calls for a semantic rather than syntactic comparison~\cite{MaozRR14j}. These semantic comparisons, e.g., as explored for Alloy~\cite{RingertW20}, may determine semantic equality despite syntactic changes, refinement of specifications, and concrete witnesses of changes.

To summarize, one challenge in evolving specifications is to discover and faithfully represent changes to the system or its environment in specifications. 
%Approaches include \cite{0041035} 
Another challenge is the analysis of changes to specifications to explore and understand the impact of changes to specifications.

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Competing Demands and Other Agents' Behaviour}
\noindent\textbf{[Responsible Author:  Greg Chance]}\\
\noindent \textbf{[Source: Breakout Room-1 (Subramanian Ramamoorthy, Yiannis Demiris, Greg Chance, Shane Windsor)]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
\begin{itemize}
 	\item \textbf{Specifying competing demands and other agents' behaviour}
 \end{itemize}
% Subramanian Ramamoorthy to summarise the outcome of their breakout room mentioned that they looked into various aspects of what do you want to do about other agents in interactions of.. in particular we were talking about what happens with autonomous vehicles. That is not the exclusive use case you can think about domestic robots and what happens with our people in the home. In both cases and many more, you have this difficulty of specification of behaviour involves other agents and what their behaviour is which is fundamentally unknown. Possibly also non-deterministic and random in some form.\\

% During the breakout room discussion... specification challenge: competing demands/negotiation, this conflicts with rules they are working with...

% Initial discussion around highway code and how to interpret this into a logical format that can be tested.

% Difficult to interpret rules such as "you shouldn't cause others to slow down" which requires insight into the causal effect of your driving on other drivers around you.

% Yiannis Demiris said he's be working on pedestrian problems and discussed the crowded market street problem, where there are so many people walking in the road looking at the market than any driver has a hard time making any progress.

% So what do you do, just pressing ahead slowly (against the known rules of driving) pushing against the pressure of the crowd conventional to normal driving behaviour. This becomes more case of managed conflict rather than any normal driving behaviour. 

% There are a number of ways in which we could interpret this situation one of them being game theory and another one being formal models. There is also the consideration of social convention and how to be fair in the driving situation. An aggressive driver would make more progress in this situation above a driver that is being fair and considerate.

% Greg Chance commented on how 5AI use a "3 second rule" as a performance metric for driving behaviour and that if the vehicle takes longer than three seconds to pull out at a junction or a roundabout then this will have scored badly in its performance metric.

% GC Notes:
% Trust may depend on several factors, like to pick up on some of these: assurance of operation (eg. following the rules of road conduct for an AV), confidence they provide (I can trust the AV to overtake this broken down vehicle on the side of the road rather than sitting behind it waiting to be fixed), and consideration of human values and ethics (social norms eg. drivers giving way to other drivers waiting to pull-out counter to legal entitlement). 
%
% If we are to trust autonomous systems then we must be able to specify and demonstrate each of these facets of trustworthiness. 
%
% The difficulty of specification of behaviour involves other agents and what their behaviour is which is fundamentally unknown. Possibly also non-deterministic and random in some form.

Conventional approaches to verification and validation for autonomous systems may seek to attain coverage against a set of requirements to demonstrate assurance of operation, safety or compliance with existing legal frameworks. 
%
This may take the form of a style of \emph{box ticking} exercise where behaviour is demonstrated in physical testing or simulated experiments against a set of pre-defined properties, for example. 
%
Harper et al.~\cite{harper2021safety} suggest a methodology to demonstrate compliance of an autonomous system against a set of objective rules (UK Highway Code~\cite{highwayCode}) by converting the human-readable rules into an objective set of logical computer expressions that can be monitored using existing database technology. 
%
Some authors have been successful in demonstrating other frameworks that may be suitable for logically proving correct adherence to driving rules using Higher Order Logic (HOL)~\cite{rizaldi} or Linear Temporal Logic (LTL)~\cite{esterle2020}. 
%
But there remains a challenge where these rules simply cannot be interpreted directly or where they conflict with other rules or expected behaviours. 

An example of this conflict is clearly illustrated in the balance between performance and safety in a driving scenario. Consider a car trying to make progress through a busy market square full of people slowly walking between the stalls, or a vehicle waiting to pull out at a busy roundabout junction. 
%
In these cases the safest option is to wait until the road ahead is completely clear before moving but better performance may be to \emph{inch ahead} to show others your intent and hope someone will allow you to make progress by giving way to you. 
%
Here we can see the conflict between adherence to formal \emph{safety properties} and a key \emph{mission parameter} to maximise performance.
%
In this example we can see that driving is much more than following the rules of the road correctly and that awareness of when a modified set of driving principles may come into play that are more about human traits than formal rules. 
%

Another example in the UK Highway Code demonstrates where a careful trade-off between safety and performance are required.
%
Rule 163 concerns the correct behaviour for overtaking another vehicle and states ``[after overtaking] move back to the left as soon as you can but do not cut in". 
%
Here we see the balance between a number of safety and performative properties; a) do no endanger oncoming vehicles by being in the wrong lane (safety), b) move back into the correct lane as quickly as possible (performance), and c) when moving back into the correct lane leave sufficient space for the vehicle being passed, i.e. do not `cut-in' (safety). 
%
We can see the atomic properties of Rule 163 a) and c) are actually in conflict with each other; 
%
to maximise safety of a) you would move to the correct lane and to maximise safety with respect to c) you should remain in the oncoming traffic lane for as long as possible. 
%
To mediate this, we must enact part b) to find the optimal driving behaviour without conflicting with part c). 
%
This clearly demonstrates that a trad-off between performance and safety will play a part in correct driving behaviour in this scenario. 
%
One of the key challenges is then ``How can we balance formal \emph{safety properties} against \emph{mission parameters} for optimal trustworthiness?". 

% Can demonstrate in a few driving examples where there is a conflict of competing drivers that must be resolved...crowded marketplace (pushing against the pressure of the crowd, managed conflict) - good example but maybe not that frequent..UKHC~\cite{highwayCode} for overtaking can conflict between safety and performance ("move back as soon as you can without cutting in", and maintaining minimum speed on motorways)...In these examples and aggressive driver will make more progress toward their mission goals than a more risk-adverse approach. 
%
Some authors have attempted to move the formal definitions of driving safety forward with advanced mathematical models such as Responsibility-Sensitive Safety (RSS)~\cite{shalevshwartz2018formal}, going beyond that of existing driving principles, e.g. Vienna convention~\cite{vienna}, which were never intended to be used for autonomous systems. Although this approach does make a start at challenging the dichotomy between formal rules and the reality of how people actually drive and is also written in a language that is readily interpretable by autonomous systems, it does not go far enough to ensure trustworthiness as far as our definitions extend and given the apparent need to have more than just awareness of the formal rules of driving. 
%
How then, can autonomous systems be specified taking account of other agents and their behaviours and also including a framework to resolve competition between formal safety properties and mission parameters. 

% Harper et al.~\cite{harper2021safety} suggest a methodology to demonstrate compliance of an autonomous system against a set of objective rules of the road. The UK Highway Code(UKHC)~\cite{highwayCode} is used as an exemplar document describing best driving practice for safe and mutually cooperative driving behaviour and can be considered a stable and valid reference upon which to base an assessment of operational validation. Conversion of the human-readable rules into an objective set of logical computer expressions is a non-trivial task but the methodology describes a potential a route to demonstrate safety assurance in autonomous systems. 
%
% We must also have assurance that the autonomous system is competent and can, at least, perform as good as an equivalent human in the same situation [ref - AI needs to be as good as us if not better] if not exceed out capabilities. 
%Lapses in concentration, distraction or poor judgement [ref Graham parkhurst?] account for [xx\%] of driving accidents, all of which an autonomous system should at least be more consistent if not overall safer. 
%
% The term \emph{liveness} is used in autonomous vehicles to describe the ability of the vehicle to make progress through a busy road network, at a roundabout or during an overtaking manoeuvrer. These are all good examples of where the autonomous system will aim to maximise mission performance without compromising safety. One could classify achievement of task goals (performance validation) as being orthogonal to avoidance of task hazards (safety validation) and a trustworthy system must achieve an acceptable balance between the two properties~\cite{harper2021safety}. 
%
% An example of this conflict could be at a roundabout or road junction where the AV must give way to other vehicles. In this scenario the task goal is to arrive at the goal destination without incurring unnecessary delay and the task hazards are the other road users. If the AV starts to manoeuvrer and collides with another vehicle, it has clearly has violated a safety property and failed to demonstrate trust. 
%
% Two other sub-optimal outcomes exist; the vehicle close-passes another vehicle (a near-miss event), and the vehicle waits for an unreasonably large gap in the approaching traffic before moving on. Conceptually these can be considered two opposing ends of the same spectrum, one end is dominated by avoidance of task hazards (bias to safety) and the other to minimising journey time (bias performance). 
%
% An optimal driving policy concerning just these two properties can therefore be considered careful between safety and performance.
%
% Can we explore this scenario as a bout of competing demands? Each road user, human or autonomous, has their own specific needs whilst driving (goals) and acceptable risks (safety). 
%

Human drivers may also have Kantian compassion for others, which is why we sometimes give-way to other drivers at busy junctions when we have right of way.\footnote{Quote from [ref]... "Immanuel Kant’s categorical imperative says that we should act only upon maxims which we could rationally generalise so that they apply equally to everyone; or in other words, only do what we could rationally want any other person to do in the same circumstances"} 
%
You could argue this is formally incorrect driving behaviour, i.e. not described in any rules or guidelines, and yet is commonly observed as a social norm in many cultures. This type of action is ostensibly counter to a self-interested agent as it does not benefit the driver in terms of their journey time or any other objective measure. 
%
So can we argue a policy which finds a solution which is "best for the world", a careful balance of safety, performance and reciprocity? 
%
If autonomous systems lived independently to humans then this argument would be largely academic, as a logical rule-set would be sufficient for optimal efficiency and co-existence between autonomous agents. 
%
Co-existence with humans may be constrained or limited in certain areas, e.g. drone air corridors, but unavoidable in others, pedestrians and cyclists with AVs.
%
Game theory may offer insight along with a means of exploring or testing independent agents that have competing demands to find optimal solutions. 
%
However, no such rulebook for social norms for driving has been posited as this is a uniquely human trait derived from the principles of inclusive  fitness and social rationality [gintis]. 
% 
The framework of game theory~\cite{schecter2016game} could be useful to understand the implications of more nuanced behaviour such as reciprocity, fairness~\cite{gintis2006foundations} and social convention and may help us specify for contexts which rely more on interpreting human behaviour (that may seem random, inconsistent or even irrational) than logical rules or safety properties. 
%
A unique challenge to the development of specifications for autonomous systems then, is the creation of objective rule-sets that can be expressed into a machine readable form without ambiguity. What is also needed is an understanding of the acceptable trade-off between seemingly orthogonal properties such as safety and performance if autonomous systems are to co-exit with humans in a responsible and trustworthy way. 
%
A final challenge, and perhaps the most ambitions, is the specification of human values and ethics and the interplay of these rules with the more objective properties of safety and performance.






%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Role of Codification in Open Environments}
\noindent\textbf{[Responsible Author:  Shane Windsor]}\\
\noindent \textbf{[Source: Breakout Room-1 (Subramanian Ramamoorthy, Yiannis Demiris, Greg Chance, Shane Windsor)]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
\begin{itemize}
	\item \textbf{Role of code/codification in open environments and how to specify them?}
\end{itemize}
Subramanian Ramamoorthy to summarise the outcome of their breakout room mentioned: the other issue that came up for us was there are some domains that are amenable to there being a code and codification and there are some that are not. So mobility is a nice thing in some sense is that there is a desire for people to follow a code and with very rare there is an exception you go through a licensing regime that make sure everyone is somewhat aware of the code. Even if they were violating it, there is reason to believe those models exist and you can make some progress with that.
If you think about home robots in particular we were thinking about a Roomba vacuum cleaner and a  3 year old child, you can't make the 3 year old to do a particular thing or there is no guarantee of compliance. So what should the robot actually do in that case? It could take a very conservative stance but then that could be bad for functionality and there nothing happens.
It could try to nudge? and play a game as if were but that could become more complex.
So we discussed aspects of this. 
Certainly there is business of what do we want to do in an open environments what is the role of the code and what are the specifications in that kind of setting... 

During the breakout room session, Shane Windsor mentioned there may be a case to codify the others behaviour but there is a question on how this can be achieved we cannot regulate all other people or have models to predict how they will behave. 
Greg said that we can only look at this situation in terms of rational agents else we end up with an Infinity problem where we postulate every possible human error that could occur. 
So then we come back to the issue of being overly compliant and we end up making no progress in the market problem. 
Subramanian Ramamoorthy discussed an issue with his Roomba vacuum cleaner and his three year old son. 
This autonomous agent cannot have a model of the behaviour of his three year old son and therefore it must take a conservative approach to its behaviour, always taking the less risky approach. 
Subramanian Ramamoorthy also mentioned a hierarchy of safety concerns where the pinnacle of this hierarchy would be something like avoid collisions at all costs and below this might be to observe red lights but not at the expense of colliding with a pedestrian. Below this we may have social conventions and an area that is more grey in terms of the interpretation of rules (trolley problem). 
One of the issues with pedestrians in this context is that they are poorly defined in terms of their predictable behaviour essentially being somewhat random within some loosely defined boundaries. 

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Trusting System after Testing/Verification/Simulation}
\noindent\textbf{[Responsible Author:  Kerstin Eder]}\\
\noindent \textbf{[Source: Breakout Room-2 (Amel Bennaceur, Anastasia Kordoni, Adrian Bodenmann, Kerstin Eder)]}\\\\
\noindent\textbf{\textit{Author Guidelines: Word count: 300-580 (maximum)}}\\
\begin{itemize}
	\item \textbf{Why is the system produced still not trusted once it is tested/verified/simulated?}
\end{itemize}
Amel Bennaceur to summarise the outcome of their breakout room mentioned: taking us back to rethinking trustworthiness, so once you test / verify / simulate why people still don't trust the system you produced. So that kind of understanding different facets of trustworthiness is also very important. 

%%------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Conclusion}

\begin{acks}
This article is a result of the fruitful discussions at the Specifying for Trustworthiness workshop held in conjunction with the Trustworthy Autonomous Systems (TAS) All Hands Meeting. The authors thank all the speakers and fellow participants, and the TAS Hub and EPSRC for their support.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{TAS-Spec-Bibliography}

\end{document}
\endinput
